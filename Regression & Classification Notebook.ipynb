{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# <center>Regression & Classification Session (with some other AI & Data Science content)</center>\n",
    "## <center>TSTT Data Science Initiative</center>\n",
    "\n",
    "<center>Presented by Rafael Guerrero</center>\n",
    "\n",
    "<center>Navin Dookeram, Darren Ramsook, Mariella Rivas, Gabriela Sewdhan</center>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### In this tutorial, we will be covering:\n",
    "\n",
    "<br>\n",
    "\n",
    "1. Some Common Definitions in AI & Data Science\n",
    "\n",
    "\n",
    "\n",
    "2. Regression <br>\n",
    "    i) Simple Linear Regression <br>\n",
    "    ii) Multiple Linear Regression <br>\n",
    "    iii) Multiple Regression\n",
    "\n",
    "\n",
    "\n",
    "3. Classification <br>\n",
    "    i) Generalised Linear Models <br>\n",
    "    ii) Logistic Regression <br>\n",
    "    \n",
    "    \n",
    "4. Feature Selection <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### 1. Some Common Definitions in AI & Data Science:\n",
    "\n",
    "<br>\n",
    "\n",
    "**Artificial Intelligence (AI):** Artifical intelligence can be disaggregated into two (2) separate categories or ideas:\n",
    "\n",
    "1) **Artificial General Intelligence (AGI)** - AGI refers to machines that mimic all the tasks that humans can do and perhaps perform the tasks even better.\n",
    "\n",
    "2) **Artificial Narrow Intelligence (ANI)** - ANI refers to machines that handle a single task. Examples of these include a smart speaker, a self-driving car, a recommender system or a system that identifies product defects in a production line.\n",
    "\n",
    "<br>\n",
    "Most of the progress these days have been in ANIs and not necessarily in AGIs, so you can rest easy knowing that there will be no robot invasions coming any time soon.\n",
    "\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image  # To input images in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"Images for Regression & Classification Session/irobot-invasion.jpg\",width=300,height=300)\n",
    "# Source: https://images.app.goo.gl/uA3vqks4omKWuDESA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "**Machine Learning** is a tool used in AI that gives the computers the ability to learn without being explicitly programmed to do so. It can be classified under three (3) categories or procedures:\n",
    "\n",
    "1) **Supervised Learning** - In supervised learning, we are aware of the values or labels of the outputs for given inputs from the previous data that we have. The machine learns the input to output mappings based on these past records to predict future outputs of new data. One use case example is an email spam filter which detects, given an input of emails, what is spam and what is not spam. Previous data would be already classified emails as spam or not spam. Examples of supervised learning procedures include linear regression, multiple regression and logistic regression.\n",
    "\n",
    "2) **Unsupervised Learning** - In unsupervised learning, we are not aware of the values or labels of the outputs for given inputs from the previous data that we have. We use the data to infer clusters or groups of the data-points by similar properties that are shared in the dataset and in some cases detect anomalies. Examples of unsupervised learning procedures include K-means clustering, hierarchical clustering, db scan clustering and anomaly detection.\n",
    "\n",
    "3) **Reinforcement Learning** - reinforcement learning is semi-supervised learning in that some part of the data is labelled, and some part is not labelled for future actions. The model learns slowly by seeing past data and will learn and then apply its knowledge to the new data as it comes up. These procedures are applied to systems that perform games like chess and backgammon, and even in robotics. The machine will know what to do next but there is uncertainty about the future until the previous task is performed.\n",
    "\n",
    "<br>\n",
    "Supervised learning procedures are the most commonly used ones.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Quick Quiz 1:**\n",
    "*Another use case example of supervised learning is pricing or valuing houses based on their aesthetics. What would you consider as the inputs and outputs in this scenario? You can give some examples for inputs.*\n",
    "\n",
    "**Quick Quiz 2:**\n",
    "*What is a use case example of unsupervised learning?*\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "The input data can be structured, unstructured or semi-structured. Structured data can be seen as data that is organised in tables. For example, data that you would see in relational databases. Unstructured data would be data that is not organised in tables. Examples include images, audio records, video records and text records. There is also semi-structured data which comprises of some elements of structured data and some elements of unstructured data.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Quick Quiz 3:**\n",
    "*Would you classify the content in the body of emails as structured or unstructured data?*\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Data Science** - According to Wikipedia, “data science is an inter-disciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from structured and unstructured data.” Data Science generally assesses hypotheses that are made on data for making business decisions. For instance, in assessing the value of houses you might notice that more bedrooms (within a certain range) may be more valuable than less bedrooms, even though the size of the house is the same. With further analyses of accounting for the extra cost to build extra bedrooms, a construction company can see what type of houses to build or a realtor will know how to value these houses, etc.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Data Mining** – According to Wikipedia, “data mining is the process of discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems.”\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Deep Learning or Neural Networks** – Deep learning, also known as neural networks or artificial neural networks, is an effective machine learning algorithm that has applications in each of the learning procedures.\n",
    "\n",
    "Some neural network that are used for supervised learning include:\n",
    "- Artificial Neural Networks (for data in numbers or the data is structured)\n",
    "- Convolutional Neural Networks (for natural language processing, and image and video recognition)\n",
    "- Recurrent Neural Networks (for time-series, handwriting and speech recognition)\n",
    "\n",
    "Some neural networks that are used for unsupervised learning include:\n",
    "- Autoencoders\n",
    "- Deep Belief Nets\n",
    "- Hebbian Learning\n",
    "- Generative adversarial networks\n",
    "- Self-organizing map\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Sources:**\n",
    "- Andrew NG: AI for everyone, Coursera (https://www.coursera.org/learn/ai-for-everyone/home/welcome)\n",
    "- Krish Naik (https://www.youtube.com/watch?v=k2P_pHQDlp0)\n",
    "- Wikipedia (https://en.wikipedia.org/wiki/Data_science) (https://en.wikipedia.org/wiki/Data_mining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"Images for Regression & Classification Session/AI-Venn-diagram.png\",width=400,height=400)\n",
    "# Source: https://gmggroup.org/how-are-ai-machine-learning-big-data-deep-learning-and-data-science-interconnected/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "**Quick Quiz 1:**\n",
    "*Another example of a supervised learning procedure is valuing houses based on their aesthetics. What examples would be considered as the inputs and outputs?*\n",
    "\n",
    "\n",
    "<font color='green'>\n",
    "\n",
    "**Answer 1:** Some inputs could be house features such as location rank, size of land in square feet, size of house in square feet, number of rooms, etc. All of these can be represented in tabular form. The outputs would be the price of the houses.\n",
    "\n",
    "    \n",
    "<br>\n",
    "\n",
    "<font color='black'>\n",
    "\n",
    "**Quick Quiz 2:**\n",
    "*What is a use case example of an unsupervised learning procedure?*\n",
    "\n",
    "<font color='green'>\n",
    "\n",
    "**Answer 2:** For instance, you may have a group of animals without knowing which families they belong to. Based on the input properties that we may have on them such as whether they could fly, whether they could swim, type of skin, etc. they can be classfied into certain classes. We can even define the number of classes or groups for the machine learning algorithm to cluster into. Another example could be classifying customers from a company into various groups. The groups would depend on the features of the customers that are used.\n",
    "\n",
    "<br>\n",
    "\n",
    "<font color='black'>\n",
    "\n",
    "**Quick Quiz 3:**\n",
    "*Would you classify the content in the body of emails as structured or unstructured data?*\n",
    "\n",
    "<font color='green'>\n",
    "\n",
    "**Answer 3:** The text of emails would be considered as unstructured data. Emails on a whole can be considered as semi-structured data since there are some structured elements such as sender email address, receiver email address, date sent, time sent, subject, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "**For this lesson, we will focus on some supervised learning procedures under generalized linear models, particularly:**\n",
    "- **Regression Analysis**\n",
    "- **Classification Analysis under Generalised Linear Models**\n",
    "\n",
    "<br>\n",
    "\n",
    "As these are supervised learning procedures, we have subsets of the data which could be regarded as output and input data. The main goal is to model the output data with the input data. The output is a dependent variable and the inputs are one or more independent variables. The main difference between regression analysis and classification analysis is that the output in regression analysis is a continuous variable, whereas the output variable in classification analysis is a discrete variable or can represent labels.\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "**Quick Quiz 4:**\n",
    "*Which of linear regression and classification could be considered for the following use cases:*\n",
    "<br>\n",
    "    i) email spam filter <br>\n",
    "    ii) valuation of houses based on aesthetics\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Quick Quiz 4:**\n",
    "*Which of linear regression and classification could be considered for the following use cases:*\n",
    "<br>\n",
    "    i) email spam filter <br>\n",
    "    ii) valuation of houses based on aesthetics\n",
    "\n",
    "<font color='green'>\n",
    "\n",
    "**Answer 4:**\n",
    "<br>\n",
    "    i) Classification. The output of either 'spam' or 'not spam' only takes two possible values; so it is discrete. <br>\n",
    "    ii) Linear Regression. The output of prices is continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### 2 Regression Analysis\n",
    "\n",
    "According to Wikipedia, \"regression analysis is a set of statistical processes for estimating the relationships between a dependent variable (often called the 'outcome variable') and one or more independent variables (often called 'predictors', 'covariates', or 'features').\"\n",
    "\n",
    "For regression analysis, some use the term, **general linear models**, in the broad scope of things, if there are multiple dependent variables. These variables are correlated (generally a change in one of these affects the others) and hence, multivariate regression analysis would be performed.\n",
    "\n",
    "In this section, we will focus primarily on linear regression where there is only one (1) dependent variable.\n",
    "\n",
    "In the classification section, we will discuss the broader sense of **generalised linear models** with which general linear models fall under. The terminology may sound confusing now but hopefully by the end of the session we should be able to have a better sense of things.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "#### Linear Regression:\n",
    "\n",
    "Linear Regression: In a nutshell, we look to take input value(s) and map them on the set of real-numbers. The output is a continuous variable, which is what we want to predict given the input value(s). The hard part is knowing how to model the data (which variables to keep, what type of relationships to use, etc.) and that is why different models are chosen. Examples include predicting stock prices, predicting market value of houses, etc.\n",
    "\n",
    "Linear regression falls into the category of general linear models with which there is only one dependent variable. The dependent variable is modelled based on none, one or more than one independent variables. In addition, the dependent variable is continuous, while each independent variable can be either continuous or categorical.\n",
    "\n",
    "<br>\n",
    "\n",
    "As a result, in linear regression, we generally condition the dependent variable, $Y$, on the independent variable(s), $X$, in the form of $Y|X$.\n",
    "It should be noted however, that it can be common to just write $Y$ but the understanding is that we are conditioning $Y$ on $X$.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Linear Regression Model**\n",
    "\n",
    "A linear regression model has the form:\n",
    "\\begin{equation}\n",
    "    Y = β_{0} + β_{1} X_{1} + ... + β_{p} X_{p} + ϵ\n",
    "    \\label{eq:linearregmodel} \\tag{1}\n",
    "\\end{equation}\n",
    "\n",
    "where $Y$ is the dependent variable\n",
    "<br>\n",
    "$X_{1}$, $X_{2}$, ..., $X_{p}$ are the p independent variables\n",
    "<br>\n",
    "$β_{0}$, $β_{1}$, ..., $β_{p}$ are unknown coefficient parameters\n",
    "<br>\n",
    "and $ϵ$ is the random error or true residual which is normally distributed with mean, 0, and variance, $σ^{2}$.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Interpretation of the Coefficients in Linear Regression**\n",
    "\n",
    "**Quick Quiz 5:**\n",
    "*What do the coefficients, $β_{0}$ and $β_{j}$, for j = 1,..., p mean?*\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Quick Quiz 5:**\n",
    "*What do the coefficients, $β_{0}$ and $β_{j}$, for j = 1,..., p mean?*\n",
    "\n",
    "<font color='green'>\n",
    "\n",
    "**Answer 5:**\n",
    "<br>\n",
    "$β_{0}$ gives us the expected value of $Y$ (or $Y|X$) either when the all the $X_{j}$'s are 0, all the $β_{j}$'s are 0 or sum of all the $β_{j}X_{j}$'s for j = 1, ..., p are 0.\n",
    "\n",
    "The definition of $β_{j}$ may vary depending on whether $X_{j}$ is a continuous variable or categorical variable.\n",
    "\n",
    "**When we have Continuous Independent Variables**\n",
    "\n",
    "When $X_{j}$ is continuous, $β_{j}$ for j = 1, ..., p tells us that a one unit increase in $X_{j}$, holding all other units constant, would result in an expected increase in $Y$ (or $Y|X$) by $β_{j}$ units.\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "If we take the expectation of model $\\eqreg{eq:linearregmodel} above, we get:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "    E(Y|X) = β_{0} + β_{1} X_{1} + ... + β_{p} X_{p}\n",
    "    \\label{eq:expectlinearregmodel} \\tag{2}\n",
    "\\end{equation}\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Let's use an example of equation $\\eqref{eq:linearregmodel}$ above with p = 2 and let $β_{0}$ = 0.45, $β_{1}$ = 2.37 and $β_{2}$ = 5.3. Now, we will have the following model:\n",
    "\n",
    "\\begin{equation}\n",
    "    Y = 0.45 + 2.37 X_{1} + 5.3 X_{2} + ϵ  \n",
    "\\end{equation}\n",
    "\n",
    "<br>\n",
    "\n",
    "The expectation would result in:\n",
    "\n",
    "\\begin{equation}\n",
    "    E(Y|X) = 0.45 + 2.37 X_{1} + 5.3 X_{2}\n",
    "\\end{equation}\n",
    "\n",
    "Thus,\n",
    "<br>\n",
    "$β_{0}$ = 0.45 gives us the expected value of $Y$ (or $Y|X$) when all $X_{j}$'s are 0.\n",
    "\\begin{equation}\n",
    "    E(Y|β_{1}X_{1}+β_{2}X_{2}=0) = 0.45\n",
    "\\end{equation}\n",
    "\n",
    "<br>\n",
    "\n",
    "$β_{1}$ = 2.37 tells us that a one unit increase in $X_{1}$, holding all other units constant, would result in an expected increase in $Y$ (or $Y|X$) by 2.37 units.\n",
    "\n",
    "\\begin{equation}\n",
    "    E(Y|X_{1}+1, X_{2}) - E(Y|X_{1}, X_{2}) = 2.37\n",
    "\\end{equation}\n",
    "\n",
    "<br>\n",
    "\n",
    "$β_{2}$ = 5.3 tells us that a one unit increase in $X_{2}$, holding all other units constant, would result in an expected increase in $Y$ (or $Y|X$) by 5.3 units. \n",
    "\n",
    "\\begin{equation}\n",
    "    E(Y|X_{1}, X_{2}+1) - E(Y|X_{1}, X_{2}) = 5.3\n",
    "\\end{equation}\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**When we have Categorical Independent Variables**\n",
    "\n",
    "When we have independent categorical variables, we can assign numerical values to it. We assign dummy variables in these cases which can generally only take two values, 0 or 1. For instance, male=0/female=1 or no=0/yes=1, etc. The dummy variable acts like an on and off switch for an option. Usually, when one switch is on, the other switches are off. \n",
    "\n",
    "<br>\n",
    "\n",
    "If we were to cater for a categorical variable with more than two options, say the colour options: platinum, gold, silver or bronze, which has 4 options, multiple dummy variables would be assigned to represent these scenarios in the model. A general rule of thumb for i options in a categorical variable is to use i-1 dummy variables if the intercept variable is available. Luckily, in programming these models, some software packages and code can do this for you automically, so you can generally just assign the names for each option in a vector.\n",
    "\n",
    "For the colour example, suppose we have a dependent variable Y and the colour options are the only other variable in question. Then, by having 4 options, we will need 3 variables apart from the intercept variable. Each of these 4 variables would represent a colour option, and the other variables would be set to 0. So,\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "    Y = β_{0} + β_{1} X_{1} + β_{2} X_{2} + β_{3} X_{3} + ϵ\n",
    "\\end{equation}\n",
    "and\n",
    "\\begin{equation}\n",
    "    E(Y|X) = β_{0} + β_{1} X_{1} + β_{2} X_{2} + β_{3} X_{3}\n",
    "\\end{equation}\n",
    "\n",
    "where $β_{0}$ represents the expected value of $Y$ when we choose 1 colour, say platinum, provided that $X_{1}$, $X_{2}$ and $X_{3}$ are set to 0.\n",
    "$β_{0} + β_{1}$ represents the expected value of $Y$ when we choose another colour, say gold, provided that $X_{1}$ is set to 1, and $X_{2}$ and $X_{3}$ are set to 0.\n",
    "$β_{0} + β_{2}$ represents the expected value of $Y$ when we choose another colour, say silver, provided that $X_{2}$ is set to 1, and $X_{1}$ and $X_{3}$ are set to 0.\n",
    "$β_{0} + β_{3}$ represents the expected value of $Y$ when we choose another colour, say bronze, provided that $X_{3}$ is set to 1, and $X_{1}$ and $X_{2}$ are set to 0.\n",
    "\n",
    "Notice that we always use the $β_{0}$ value for each colour option. As a result, the value of the other $β$'s depend on this value. In general, the $β_{j}$'s for j = 1, ..., p represent the expected change in value of $Y$ when we choose the j$^{th}$ additional colour opposed to the base colour (associated with $β_{0}$), holding all other variables constant.\n",
    "\n",
    "<br>\n",
    "\n",
    "<font color='red'>\n",
    "\n",
    "**Note:** We can have models with both continuous and categorical independent variables.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<font color='black'>\n",
    "\n",
    "\n",
    "**Applying Obervations to the Model**\n",
    "\n",
    "The data to fit the model would consist of n independent observations of $Y$ with corresponding $X_{j}$'s. We will now have the following model:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "    Y_{i} = β_{0} + β_{1} x_{i1} + ... + β_{p} x_{ip} + ϵ_{i}\n",
    "    \\label{eq:obslinearregmodel1} \\tag{3}\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "<br>\n",
    "i = 1, ..., n\n",
    "<br>\n",
    "$Y_{i}$ is the dependent variable for the i$^{th}$ observation\n",
    "<br>\n",
    "$x_{i1}$, $x_{i2}$, ..., $x_{ip}$ are the p independent variables for the i$^{th}$ observation\n",
    "<br>\n",
    "$β_{0}$, $β_{1}$, ..., $β_{p}$ are unknown parameters\n",
    "<br>\n",
    "and $ϵ_{i}$ is the i$^{th}$ error term\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Assumptions of the Linear Regression Model**\n",
    "\n",
    "The assumptions of the linear regression model are:\n",
    "<br>\n",
    "$ϵ_{i}$'s are independent and have identically Normal distributions with mean, μ = 0 and constant variance, $σ^{2}$\n",
    "<br>\n",
    "i.e. $ϵ_{i}$'s ~ i.i.d. N(0, $σ^{2}$)\n",
    "<br>\n",
    "across all observations in the population.\n",
    "\n",
    "<br>\n",
    "From these assumptions:\n",
    "<br>\n",
    "\n",
    "- The observations should be independently conducted. We test independence of observations by plotting residuals, $\\hat{ϵ}_{i}$ vs time, i (order, i, in which the n variables were obtained).\n",
    "<br>\n",
    "\n",
    "- A fancy term used in place of constant variance is *homoscedasticity*. This just means that the variance, $σ^{2}$, is the same across all observations. We test constant variance by plotting residuals, $\\hat{ϵ}_{i}$ vs fitted values, $\\hat{Y}_{i}$ and determine if there is a constant horizontal band of points centred around 0. This plot can also detect cases of non-linearity in the model (at least one independent variable has a non-linear relationship with the dependent variable), but LOESS (or LOWESS) curves in this plot are probably better to include as well. When the homoscedasticity assumption is violated, we end up getting heteroscedasticity (non-constant variance across observations). Heteroscedasticity is troublesome because:\n",
    "    1) some observations are less reliable than others and should be down-weighted in the fitting procedure.\n",
    "    2) it is a sign that ordinary least-squares estimation (OLS) is not optimal and we can do better by switching to weighted least-squares (WLS).\n",
    "    3) it can distort standard errors (deviations) of the estimates.\n",
    "<br>\n",
    "\n",
    "- Moderate departures from normality may not significantly impact linear regression to work effectively, especially if the sample size is large. We test normality using normal q-q plots (sample quantile vs theoretical quantile) or hypothesis tests like the Shapiro–Wilks test. For the normal q-q plots, we should get a diagonal alignment of points. “Rather than testing the null hypothesis of normality-which is false anyway-we should focus on whether the departures from normality exhibited by our data are serious enough to adversely impact the procedures that we want to use.” - STAT 6120 Lecture 2 Notes\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Alternative Notation for Linear Regression Model & Converting the Equations to Vectors and Matrix Form**\n",
    "\n",
    "For notational reasons, it may be more convenient to write the model as:\n",
    "\\begin{equation}\n",
    "    Y_{i} = β_{1} x_{i1} + β_{2} x_{i2} + ... + β_{p} x_{ip} + ϵ_{i}\n",
    "    \\label{eq:obslinearregmodel2} \\tag{4}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "where we can assign the value of 1 for $x_{i1}$ for all i.\n",
    "\n",
    "<br>\n",
    "\n",
    "<font color='red'>\n",
    "\n",
    "**Note:** If we convert $\\eqref{eq:obslinearregmodel1}$ to $\\eqref{eq:obslinearregmodel2}$ with a defined value for p in $\\eqref{eq:obslinearregmodel1}$, say p = 5, it will be p+1 in $\\eqref{eq:obslinearregmodel2}$, i.e. p+1 = 6.\n",
    "\n",
    "<br>\n",
    "<font color='black'>\n",
    "\n",
    "\n",
    "With this notation, we can write the dependent variable, independent variables and coefficients into vectors and matrices:\n",
    "\n",
    "<br>\n",
    "\\begin{equation}\n",
    "    Y = \\begin{bmatrix}\n",
    "    Y_{1} & Y_{2} & ... & Y_{n}\n",
    "    \\end{bmatrix}\n",
    "    ^{T}\n",
    "    \\label{eq:Yvec} \\tag{5}\n",
    "\\end{equation}\n",
    "\n",
    "<br>\n",
    "\n",
    "\\begin{equation}\n",
    "    β = \\begin{bmatrix}\n",
    "    β_{1} & β_{2} & ... & β_{p}\n",
    "    \\end{bmatrix}\n",
    "    ^{T}\n",
    "    \\label{eq:βvec} \\tag{6}\n",
    "\\end{equation}\n",
    "\n",
    "<br>\n",
    "\n",
    "\\begin{equation}\n",
    "    X = \\begin{bmatrix}\n",
    "    x_{11} & x_{12} & ... & x_{1p}\\\\\n",
    "    x_{21} & x_{22} & ... & x_{2p}\\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\vdots\\\\\n",
    "    x_{n1} & x_{n2} & ... & x_{np}\\\\\n",
    "    \\end{bmatrix}\n",
    "    = \\begin{bmatrix}\n",
    "    x_{1}^T\\\\\n",
    "    x_{2}^T\\\\\n",
    "    \\vdots\\\\\n",
    "    x_{n}^T\\\\\n",
    "    \\end{bmatrix}\n",
    "    = \\begin{bmatrix}\n",
    "    X_{1} & X_{2} & ... & X_{p}\n",
    "    \\end{bmatrix}\n",
    "    \\label{eq:Xvec} \\tag{7}\n",
    "\\end{equation}\n",
    "\n",
    "<br>\n",
    "\n",
    "\\begin{equation}\n",
    "    x_{i}^{T} = \\begin{bmatrix}\n",
    "    x_{i1} & x_{i2} & ... & x_{ip}\n",
    "    \\end{bmatrix}\n",
    "    \\label{eq:xvec} \\tag{7}\n",
    "\\end{equation}\n",
    "\n",
    "<br>\n",
    "where $x_{i}$ is the p $\\times$ 1 vector of predictors for observation i, for i = 1, ..., n\n",
    "<br>\n",
    "and $X_{j}$ is the n $\\times$ 1 vector containing the values of the j$^{th}$ predictor for units j = 1, ..., p\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "We can write the model as:\n",
    "<center> $Y|X$ ~ N($Xβ$, $σ^{2}I$) </center>\n",
    "<br>\n",
    "where $I$ is the identity matrix.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Quick Quiz 6:**\n",
    "*What are the dimensions of $X$, $Xβ$ and $σ^{2}I$?*\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Quick Quiz 6:**\n",
    "*What are the dimensions of $X$, $Xβ$ and $σ^{2}I$?*\n",
    "\n",
    "<font color='green'>\n",
    "\n",
    "**Answer 6:**\n",
    "<br>\n",
    "$X$ would be a n $\\times$ p matrix, $Xβ$ would be a n $\\times$ 1 vector and $σ^{2}I$ would be a n $\\times$ n matrix represented as:\n",
    "\n",
    "\\begin{equation}\n",
    "    X = \\begin{bmatrix}\n",
    "    x_{11} & x_{12} & ... & x_{1p}\\\\\n",
    "    x_{21} & x_{22} & ... & x_{2p}\\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\vdots\\\\\n",
    "    x_{n1} & x_{n2} & ... & x_{np}\\\\\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    Xβ = \\begin{bmatrix}\n",
    "    β_{1} x_{11} + β_{2} x_{12} + ... + β_{p} x_{1p}\\\\\n",
    "    β_{1} x_{21} + β_{2} x_{22} + ... + β_{p} x_{2p}\\\\\n",
    "    \\vdots\\\\\n",
    "    β_{1} x_{n1} + β_{2} x_{n2} + ... + β_{p} x_{np}\\\\\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{equation}\n",
    "    σ^{2}I = σ^{2}\\begin{bmatrix}\n",
    "    1 & 0 & ... & 0\\\\\n",
    "    0 & 1 & ... & 0\\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "    0 & 0 & ... & 1\\\\\n",
    "    \\end{bmatrix}_{n \\times n}\n",
    "    = \\begin{bmatrix}\n",
    "    σ^{2} & 0 & ... & 0\\\\\n",
    "    0 & σ^{2} & ... & 0\\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "    0 & 0 & ... & σ^{2}\\\\\n",
    "    \\end{bmatrix}_{n \\times n}\n",
    "\\end{equation}\n",
    "\n",
    "<font color='black'>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Quick Quiz 7:**\n",
    "*How would you write the vectors and matrices of $Y$, $β$, $X$ and $Xβ$ using equation $\\eqref{eq:obslinearregmodel1}$?*\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Quick Quiz 7:**\n",
    "*How would you write the vectors and matrices of $Y$, $β$, $X$, $x_{i}^T$ and $Xβ$ using equation $\\eqref{eq:obslinearregmodel1}$?*\n",
    "\n",
    "<font color='green'>\n",
    "\n",
    "**Answer 7:**\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Recall, equation is $\\eqref{eq:obslinearregmodel1}$:\n",
    "\\begin{equation}\n",
    "    Y_{i} = β_{0} + β_{1} x_{i1} + ... + β_{p} x_{ip} + ϵ_{i}\n",
    "\\end{equation}\n",
    "\n",
    "So,\n",
    "\n",
    "<br>\n",
    "\n",
    "\\begin{equation}\n",
    "    Y = \\begin{bmatrix}\n",
    "    Y_{1} & Y_{2} & ... & Y_{n}\n",
    "    \\end{bmatrix}\n",
    "    ^{T}\n",
    "\\end{equation}\n",
    "\n",
    "<br>\n",
    "\n",
    "\\begin{equation}\n",
    "    β = \\begin{bmatrix}\n",
    "    β_{0} & β_{1} & ... & β_{p}\n",
    "    \\end{bmatrix}\n",
    "    ^{T}\n",
    "\\end{equation}\n",
    "\n",
    "<br>\n",
    "\n",
    "\\begin{equation}\n",
    "    X = \\begin{bmatrix}\n",
    "    1 & x_{11} & x_{12} & ... & x_{1p}\\\\\n",
    "    1 & x_{21} & x_{22} & ... & x_{2p}\\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\\n",
    "    1 & x_{n1} & x_{n2} & ... & x_{np}\\\\\n",
    "    \\end{bmatrix}\n",
    "    = \\begin{bmatrix}\n",
    "    x_{1}^T\\\\\n",
    "    x_{2}^T\\\\\n",
    "    \\vdots\\\\\n",
    "    x_{n}^T\\\\\n",
    "    \\end{bmatrix}\n",
    "    = \\begin{bmatrix}\n",
    "    1_{n \\times 1} & X_{1} & X_{2} & ... & X_{p}\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "<br>\n",
    "\n",
    "\\begin{equation}\n",
    "    Xβ = \\begin{bmatrix}\n",
    "    β_{0} + β_{1} x_{11} + β_{2} x_{12} + ... + β_{p} x_{1p}\\\\\n",
    "    β_{0} + β_{1} x_{21} + β_{2} x_{22} + ... + β_{p} x_{2p}\\\\\n",
    "    \\vdots\\\\\n",
    "    β_{0} + β_{1} x_{n1} + β_{2} x_{n2} + ... + β_{p} x_{np}\\\\\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<font color='black'>\n",
    "\n",
    "**Method of Ordinary Least-Squares to fit the Model**\n",
    "\n",
    "One of the main goals in regression is to find the value of $β$ that makes\n",
    "\\begin{equation}\n",
    "    Xβ = β_{1} X_{1} + β_{2} X_{2} + ... + β_{p} X_{p}\n",
    "\\end{equation}\n",
    "close to Y.\n",
    "\n",
    "<br>\n",
    "\n",
    "One approach is to use *the method of ordinary least-squares*. For this method, we need to find the $\\hat{β}$ for which\n",
    "\\begin{equation}\n",
    "    \\hat{Y} = X\\hat{β}\n",
    "\\end{equation}\n",
    "will minimise the Euclidean distance between $Y$ and $\\hat{Y}$; i.e. minimise ||$\\hat{ϵ}$||, where $\\hat{ϵ}$ = $Y$ - $\\hat{Y}$. We are essentially finding the $\\hat{β}$ for which $\\hat{Y}$ is the projection of $Y$ on the linear space spanned by the columns of $X$ (i.e. $X_{1}, ..., X_{p}$).\n",
    "\n",
    "<br>\n",
    "\n",
    "In the proof, we use ||$\\hat{ϵ}$||$^2$ = ($Y$ - $\\hat{Y}$)$^{T}$($Y$ - $\\hat{Y}$), and it can be shown that\n",
    "\\begin{equation}\n",
    "    \\hat{Y} = X(X^{T}X)^{-1}X^{T}Y\n",
    "\\end{equation}\n",
    "and hence, $\\hat{Y}$ = $X\\hat{β}$ gives us,\n",
    "\\begin{equation}\n",
    "    \\hat{β} = (X^{T}X)^{-1}X^{T}Y\n",
    "    = \\begin{bmatrix}\n",
    "    \\hat{β}_{1} & \\hat{β}_{2} ... & \\hat{β}_{p}\n",
    "    \\end{bmatrix}\n",
    "    ^{T}\n",
    "\\end{equation}\n",
    "\n",
    "||$\\hat{ϵ}$||$^2$ is called the residual sum of squares or sum of squared residuals or error sum of squares.\n",
    "\n",
    "From this and $Y|X$ ~ N($Xβ$, $σ^{2}I$), we can get:\n",
    "<br>\n",
    "<center> $\\hat{β}|X$ ~ N($β$, $σ^{2}(X^{T}X)^{-1}$) </center>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will be applying Python to what we learnt.\n",
    "\n",
    "<br>\n",
    "\n",
    "Data Camp has compiled a cheat sheet for basic Python manipulation and functionalilties of some libraries that can be used in Data Science (as mentioned in the Python session). Please click the link below for more information.\n",
    "<br>\n",
    "http://www.utc.fr/~jlaforet/Suppl/python-cheatsheets.pdf\n",
    "\n",
    "<br>\n",
    "In this class, we will be using some of these libraries (Pandas, NumPy, Scikit-Learn, Matplotlib and Seaborn) in addition to others.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression(normalize=True)\n",
    "import statsmodels.stats.api as sms\n",
    "import statsmodels.api as sm\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt # for plotting\n",
    "from matplotlib.pyplot import figure\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price (millions)</th>\n",
       "      <th>House Size (ft$^{2}$)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.50</td>\n",
       "      <td>2152.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.70</td>\n",
       "      <td>4305.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.00</td>\n",
       "      <td>5166.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.00</td>\n",
       "      <td>6458.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.75</td>\n",
       "      <td>3229.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1614.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1725.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>4.60</td>\n",
       "      <td>5677.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Price (millions)  House Size (ft$^{2}$)\n",
       "0              1.50                2152.78\n",
       "1              2.70                4305.56\n",
       "2              3.00                5166.68\n",
       "3              5.00                6458.35\n",
       "4              1.75                3229.17\n",
       "5              1.00                1614.59\n",
       "6              1.30                1725.57\n",
       "7              4.60                5677.30"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housingdata = [[1.5, 2152.78], [2.7, 4305.56], [3, 5166.68 ], [5, 6458.35], [1.75, 3229.17], [1, 1614.59], [1.3, 1725.57], [4.6, 5677.30]]\n",
    "pd.DataFrame(housingdata, columns=[\"Price (millions)\", \"House Size (ft$^{2}$)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Price (millions)  House Size (ft^2)\n",
      "0              1.50            2152.78\n",
      "1              2.70            4305.56\n",
      "2              3.00            5166.68\n",
      "3              5.00            6458.35\n",
      "4              1.75            3229.17\n",
      "5              1.00            1614.59\n",
      "6              1.30            1725.57\n",
      "7              4.60            5677.30\n"
     ]
    }
   ],
   "source": [
    "housingdata = pd.DataFrame(housingdata, columns=[\"Price (millions)\", \"House Size (ft^2)\"])\n",
    "print(housingdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1)\n",
    "# Use the sns.regplot to get the regression plot:\n",
    "houseplot1 = sns.regplot('House Size (ft^2)', 'Price (millions)', label = 'Observed Value: $Y_{i}$', data=housingdata, ci=0, color = 'magenta')\n",
    "\n",
    "# To get the regression line in the legend:\n",
    "houseplot2 = sns.regplot('House Size (ft^2)', 'Price (millions)', label = 'OLS Regression Line', data=housingdata, scatter = False, ci=0, color = 'orange')\n",
    "\n",
    "# Organise the x and y house data:\n",
    "xhouse = np.array(housingdata['House Size (ft^2)']).reshape((-1,1))\n",
    "yhouse = np.array(housingdata['Price (millions)'])\n",
    "\n",
    "\n",
    "# 2)\n",
    "# Using linear regression to estimate the intercept and coefficient of xhouse:\n",
    "housemod = lr.fit(xhouse, yhouse)\n",
    "beta0 = housemod.intercept_\n",
    "beta1 = housemod.coef_\n",
    "\n",
    "# Plot the Fitted Values Yhat_i:\n",
    "yhat = beta0 + beta1 * xhouse\n",
    "plt.scatter(xhouse, yhat, color = 'blue', marker = 'p', label = \"Fitted Value: $\\hatY_{i}$\", linewidths = 2, alpha = 1)\n",
    "\n",
    "\n",
    "# 3)\n",
    "# Organise the line that takes the value of the mean of Y_i:\n",
    "domain = np.linspace(start = 1300, stop = 6700, num = 1000)\n",
    "\n",
    "price_bar = np.mean(housingdata['Price (millions)']) * np.ones((1000,1),dtype=float)\n",
    "plt.plot(domain.reshape((-1, 1)), price_bar, color='green', linestyle='dashed', label = 'Mean of $Y_{i}$', linewidth=2, markersize=1)\n",
    "\n",
    "\n",
    "# 4)\n",
    "# Plotting the dotted lines to show residuals:\n",
    "for i in range(xhouse.shape[0]):\n",
    "    yhati = beta0 + beta1 * xhouse[i]\n",
    "    if yhouse[i] <= yhati:\n",
    "        domain = np.linspace(start = yhouse[i], stop = yhati, num = 1000)\n",
    "    else:\n",
    "        domain = np.linspace(start = yhati, stop = yhouse[i], num = 1000)\n",
    "\n",
    "    xhousei = xhouse[i] * np.ones((1000,1),dtype=float)\n",
    "\n",
    "    if i == 0:\n",
    "        plt.plot(xhousei, domain.reshape((-1, 1)), color='black', linestyle=':', label = 'Residuals: $Y_{i}$ - $\\hatY_{i}$', linewidth=2, markersize=1)\n",
    "    else:\n",
    "        plt.plot(xhousei, domain.reshape((-1, 1)), color='black', linestyle=':', linewidth=2, markersize=1)\n",
    "\n",
    "# Legend location:\n",
    "plt.legend(loc='best', fontsize = 'large')\n",
    "\n",
    "# Colour of the backdrop:\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "# Size of the plot:\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "fig.set_size_inches(15, 8, forward = True)\n",
    "\n",
    "# Change the font size of the title and axes labels:\n",
    "fig.suptitle('Regression Plot of Price (millions) vs House Size (ft$^{2}$)', fontsize=18)\n",
    "plt.xlabel('House Size (ft$^{2}$)', fontsize=15)\n",
    "plt.ylabel('Price (millions)', fontsize=15)\n",
    "\n",
    "\n",
    "# function to show the plot:\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#?sns.regplot\n",
    "#?plt.plot\n",
    "#?plt.scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(x).reshape((-1, 1))\n",
    "y = np.array(y)\n",
    "\n",
    "# Using linear regression to estimate the intercept and coefficient of x.\n",
    "model = lr.fit(x, y)\n",
    "print(model.intercept_)\n",
    "print(model.coef_)\n",
    "#?np.reshape\n",
    "# Source: (https://realpython.com/linear-regression-in-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Use the formula:\n",
    "X = np.c_[ np.ones(x.shape[0]), x ]\n",
    "print('The value for the X matrix is:', X)\n",
    "beta = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T),y)\n",
    "\n",
    "print('The estimates of the coefficients are', beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "?houseplot1.set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For getting regression outout like that in R.\n",
    "https://towardsdatascience.com/verifying-the-assumptions-of-linear-regression-in-python-and-r-f4cd2907d4c0\n",
    "\n",
    "BODY DATA EXAMPLE\n",
    "\n",
    "\n",
    "Consider the body.txt file which consists of body measurements from a sample of n = 1835 adults. It contains:\n",
    "*HEIGHT* - standing height without shoes (inches)\n",
    "*WEIGHT* - body weight (pounds)\n",
    "*WAIST* - waist circumference (inches)\n",
    "*HIPS* - hips circumference (inches)\n",
    "*CHOL* - total serum cholesterol (mg/dl)\n",
    "\n",
    "We have been asked to investigate the relationship between Cholesterol (CHOL) and Body Mass Index (BMI) and Measure of Body Shape (MORPH), where:\n",
    "\n",
    "$BMI = \\frac{WEIGHT(kgs)}{ (HEIGHT (m))^{2}}$\n",
    "\n",
    "$MORPH = \\frac{WAIST}{HIPS}$\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{equation}\n",
    "    E(CHOL|BMI, MORPH) = β_{0} + β_{1} BMI + β_{2} MORPH\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the body.txt file.\n",
    "\n",
    "# Body is a .txt file with a single space delimiter.\n",
    "body = pd.read_csv('Datasets for Regression & Classification/body.txt', sep=\"\\s+\", header='infer')\n",
    "\n",
    "print(\"The dimensions of the body.txt data frame is\", body.shape, \"\\n\\n\")      # Look at the dimensions of the data frame.\n",
    "\n",
    "print(\"The first few rows of the body.txt data frame are:\\n\")\n",
    "print(body.head())     # Look at the first few rows of data.\n",
    "\n",
    "#?pd.read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to create two more variables, BMI and MORPH\n",
    "# using some variables in the dataset.\n",
    "\n",
    "\n",
    "# First, let's look at BMI:\n",
    "# BMI = WEIGHT (in kgs) / (HEIGHT (in m))^2\n",
    "\n",
    "# Notice the change in the units for WEIGHT and HEIGHT.\n",
    "# So, let's convert them:\n",
    "\n",
    "# Weight from pounds to kgs\n",
    "# 1 pound = 0.45359237 kgs\n",
    "WEIGHT_kg = 0.45359237 * body[['weight']]\n",
    "\n",
    "# Height from inches to m\n",
    "# 1 inch = 0.0254 m\n",
    "HEIGHT_m = 0.0254 * body[['height']]\n",
    "\n",
    "BMI = np.divide(WEIGHT_kg, HEIGHT_m ** 2)    # ** represents ^ in HEIGHT^2\n",
    "\n",
    "\n",
    "# Next, let's look at MORPH:\n",
    "# MORPH = WAIST / HIPS\n",
    "MORPH = np.divide(body[['waist']], body[['hips']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(WEIGHT_kg.shape)\n",
    "print(HEIGHT_m.shape)\n",
    "print(MORPH.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the Assumptions:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.c_[BMI, MORPH]\n",
    "X = pd.DataFrame(X, columns=['BMI', 'MORPH'])   # Combining the BMI and MORPH into 1 dataframe.\n",
    "X = sm.add_constant(X)   # Adding column of 1's to the X's.\n",
    "print(\"The X matrix looks like:\\n\\n\", X.head(), \"\\n\\nwith dimensions: \", X.shape)\n",
    "\n",
    "y = body[['chol']]\n",
    "\n",
    "lin_reg = sm.OLS(y,X).fit()\n",
    "lin_reg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format ='retina'\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.stats.api as sms\n",
    "\n",
    "\n",
    "def linearity_test(model, y):\n",
    "    '''\n",
    "    Function for visually inspecting the assumptions of constant variance and linearity in a linear regression model.\n",
    "    It plots Residuals vs Fitted Values with a LOESS curve.\n",
    "    \n",
    "    Args:\n",
    "    * model - fitted OLS model from statsmodels\n",
    "    * y - observed values\n",
    "    '''\n",
    "    fitted_vals = model.predict()\n",
    "    resids = model.resid\n",
    "\n",
    "    sns.regplot(x=fitted_vals, y=resids, lowess=True, line_kws={'color': 'red'})\n",
    "    plt.title('Residuals vs. Predicted Values')\n",
    "    plt.xlabel=('Fitted Values')\n",
    "    plt.ylabel=('Residuals')\n",
    "    \n",
    "linearity_test(lin_reg, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the mean of the residuals is 0.\n",
    "print(\"The mean of the residuals is: \", lin_reg.resid.mean())\n",
    "\n",
    "# Apparantly, I had to use ** to represent to the power of (^).\n",
    "if lin_reg.resid.mean() < 10 ** -5 and lin_reg.resid.mean() > -10 ** -5:\n",
    "    print(\"This seems close to 0.\")\n",
    "else:\n",
    "    print(\"This does not seem close to 0.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def normality_of_residuals_test(model):\n",
    "    '''\n",
    "    Function for drawing the normal QQ-plot of the residuals and running 4 statistical tests to \n",
    "    investigate the normality of residuals.\n",
    "    \n",
    "    Arg:\n",
    "    * model - fitted OLS models from statsmodels\n",
    "    '''\n",
    "    sm.ProbPlot(model.resid).qqplot(line='s');\n",
    "    plt.title('Q-Q plot');\n",
    "\n",
    "    jb = stats.jarque_bera(model.resid)\n",
    "    sw = stats.shapiro(model.resid)\n",
    "    ad = stats.anderson(model.resid, dist='norm')\n",
    "    ks = stats.kstest(model.resid, 'norm')\n",
    "    \n",
    "    print(f'Jarque-Bera test ---- statistic: {jb[0]:.4f}, p-value: {jb[1]}')\n",
    "    print(f'Shapiro-Wilk test ---- statistic: {sw[0]:.4f}, p-value: {sw[1]:.4f}')\n",
    "    print(f'Kolmogorov-Smirnov test ---- statistic: {ks.statistic:.4f}, p-value: {ks.pvalue:.4f}')\n",
    "    print(f'Anderson-Darling test ---- statistic: {ad.statistic:.4f}, 5% critical value: {ad.critical_values[2]:.4f}')\n",
    "    print('If the returned AD statistic is larger than the critical value, then for the 5% significance level, the null hypothesis that the data come from the Normal distribution should be rejected. ')\n",
    "    \n",
    "normality_of_residuals_test(lin_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Linear Regression, we see terms like:\n",
    "- Simple Linear Regression\n",
    "- Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "\n",
    "from scipy.stats import norm # for generating a Normal distribution\n",
    "from scipy.stats import t # for generating a t distribution\n",
    "from scipy.stats import f # for generating an F distribution\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>\n",
    "\n",
    "Look at Brandon Foltz (https://www.youtube.com/watch?v=dQNpSa-bq4M) for a guide on the different terms (estimate, model, etc.) of equations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?sns.regplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression\n",
    "- Y = β<sub>0</sub> + β<sub>1</sub>x<sub>1</sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression\n",
    "\n",
    "**Multiple Linear Regression Model:**\n",
    "- $y$ = β<sub>0</sub> + β<sub>1</sub>$x$<sub>1</sub> + ... + β<sub>p</sub>$x$<sub>p</sub> + ϵ\n",
    "\n",
    "where ϵ is the error\n",
    "<br>\n",
    "β<sub>0</sub> + β<sub>1</sub>$x$<sub>1</sub> + ... + β<sub>p</sub>$x$<sub>p</sub> is the linear equation\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Multiple Linear Regression Equation:**\n",
    "- E($y$) = β<sub>0</sub> + β<sub>1</sub>$x$<sub>1</sub> + ... + β<sub>p</sub>$x$<sub>p</sub>\n",
    "\n",
    "the expectation of the error term, E(ϵ), is assumed to be 0\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Estimated Multiple Linear Regression Equation:**\n",
    "- $\\hat{y}$ = $\\hat{β}$<sub>0</sub> + $\\hat{β}$<sub>1</sub>$x$<sub>1</sub> + ... + $\\hat{β}$<sub>p</sub>$x$<sub>p</sub>\n",
    "\n",
    "$\\hat{y}$ is the predicted value of $y$\n",
    "<br>\n",
    "$\\hat{β}$<sub>0</sub>, $\\hat{β}$<sub>1</sub>, ..., $\\hat{β}$<sub>p</sub> are estimated values of \n",
    "β<sub>0</sub>, β<sub>1</sub>, ..., β<sub>p</sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "P(Y = 1 | X) = tanh( β<sub>0</sub> + β<sub>1</sub>x<sub>1</sub> )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[0, 4.2], [0, 5.1], [0, 5.5], [1, 8.2], [1, 9.0], [1, 9.1]]\n",
    "pd.DataFrame(data, columns=[\"Admission\", \"GPA\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example of some Normal Distributions.\n",
    "\n",
    "domain = np.linspace(start = -5, stop = 5, num = 1000)\n",
    "# Return evenly spaced numbers over a specified interval.\n",
    "# Returns `num` evenly spaced samples, calculated over the interval [`start`, `stop`].\n",
    "# The endpoint of the interval can optionally be excluded.\n",
    "# ?np.linspace\n",
    "\n",
    "\n",
    "plt.plot(domain, norm.pdf(domain,loc=0,scale=np.sqrt(1)), label='μ=0, $σ^{2}$=1')\n",
    "plt.plot(domain, norm.pdf(domain,loc=0,scale=np.sqrt(0.2)), label='μ=0, $σ^{2}$=0.2')\n",
    "plt.plot(domain, norm.pdf(domain,loc=0,scale=np.sqrt(5)), label='μ=0, $σ^{2}$=5')\n",
    "plt.plot(domain, norm.pdf(domain,loc=-2,scale=np.sqrt(0.5)), label='μ=-2, $σ^{2}$=0.5')\n",
    "plt.title(\"Normal Distribution\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# ?plt.plot\n",
    "# ?norm.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of some t Distributions.\n",
    "\n",
    "domain = np.linspace(start = -5, stop = 5, num = 1000)\n",
    "# Return evenly spaced numbers over a specified interval.\n",
    "# Returns `num` evenly spaced samples, calculated over the interval [`start`, `stop`].\n",
    "# The endpoint of the interval can optionally be excluded.\n",
    "# ?np.linspace\n",
    "\n",
    "\n",
    "plt.plot(domain, norm.pdf(domain,loc=0,scale=np.sqrt(1)), label='N(μ=0, $σ^{2}$=1)')\n",
    "plt.plot(domain, t.pdf(domain,df=1,loc=0,scale=np.sqrt(1)), label='t(df=1)')\n",
    "plt.plot(domain, t.pdf(domain,df=2,loc=0,scale=np.sqrt(1)), label='t(df=2)')\n",
    "plt.plot(domain, t.pdf(domain,df=4,loc=0,scale=np.sqrt(1)), label='t(df=4)')\n",
    "plt.plot(domain, t.pdf(domain,df=10,loc=0,scale=np.sqrt(1)), label='t(df=10)')\n",
    "plt.title(\"Normal & t Distributions\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# ?plt.plot\n",
    "# ?norm.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of some F Distributions.\n",
    "\n",
    "domain = np.linspace(start = 0.01, stop = 5, num = 100000)\n",
    "# Return evenly spaced numbers over a specified interval.\n",
    "# Returns `num` evenly spaced samples, calculated over the interval [`start`, `stop`].\n",
    "# The endpoint of the interval can optionally be excluded.\n",
    "# ?np.linspace\n",
    "\n",
    "plt.plot(domain, f.pdf(domain,dfn=1,dfd=1,loc=0,scale=1), label='df$_{1}$=1, df$_{2}$=1')\n",
    "plt.plot(domain, f.pdf(domain,dfn=2,dfd=1,loc=0,scale=1), label='df$_{1}$=2, df$_{2}$=1')\n",
    "plt.plot(domain, f.pdf(domain,dfn=3,dfd=2,loc=0,scale=1), label='df$_{1}$=5, df$_{2}$=2')\n",
    "plt.plot(domain, f.pdf(domain,dfn=10,dfd=1,loc=0,scale=1), label='df$_{1}$=10, df$_{2}$=1')\n",
    "plt.plot(domain, f.pdf(domain,dfn=100,dfd=100,loc=0,scale=1), label='df$_{1}$=100, df$_{2}$=100')\n",
    "plt.title(\"F Distribution\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# ?plt.plot\n",
    "# ?f.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### 4. Feature Selection\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<font color='red'>\n",
    "\n",
    "Review this tutorial for feature selection:\n",
    "https://www.datacamp.com/community/tutorials/feature-selection-python\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<font color='black'>\n",
    "\n",
    "For both Regression and Classification projects, we will generally have to go through *feature selection* for datasets to determine the appropriate input variables to choose from.\n",
    "\n",
    "We usually start off with a manual selection of variables from the dataset and remove those that do not impact the dependent variable based on the nature of the variable and/or prior knowledge in the field. Some form of this was conducted in the ETL Session, where the bitcoin data was transformed by removing certain columns that did not seem relevant. \n",
    "\n",
    "\n",
    "**Feature Selection:** Given a dataset, we identify which independent variables are appropriate for predicting the dependent variable. This can be done through:\n",
    "<br>\n",
    "1) Wrapper methods <br>\n",
    "2) Filter methods <br>\n",
    "3) Embedded methods <br>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Feature selection helps eliminate variables that are deemed *irrelevant* or *redundant*.\n",
    "- **Irrelevant** in the sense that the variable would have no significant impact on the predicting the value of the dependent variable. For example, if we were to predict the price of a house, a variable such as the name of the construction company that built the house could be seen as irrelevant.\n",
    "\n",
    "|  | Price (millions) | House Size (ft<sup>2</sup>) | Construction Company | ... |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| 1 | 1.5 | 2152.78 | APS | ... |\n",
    "| 2 | 2.7 | 4305.56 | Harry's | ... |\n",
    "| 3 | 3 | 5166.68 | APS | ... |\n",
    "| 4 | 5 | 6458.35 | Abel | ... |\n",
    "| 5 | 1.75 | 3229.17 | JS | ... |\n",
    "| 6 | 1 | 1614.59 | JS | ... |\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Redundant** in the sense that an independent variable may be a linear combination of the other independent variables. This means that the variable is linearly dependent. Recall in the Linear Algebra Session that a vector, **v<sub>1</sub>**, is linearly dependent to vectors, **v<sub>2</sub>** and **v<sub>3</sub>** if **v<sub>1</sub>** = a · **v<sub>2</sub>** + b · **v<sub>3</sub>**, where a and b are real numbers. We call this *perfect multicollinearity* and this is generally not good for modelling. A prime example is in regression analysis where the parameters, $\\hat{β}$, are estimated through least squares method using a formula which contains the inverse of a transformation of the independent variables in matrix form (X<sup>T</sup>X)<sup>-1</sup>; this linear dependence is a problem because that would mean that (X<sup>T</sup>X)<sup>-1</sup> would not exist. An example of redundant data in the case of pricing houses could be that one variable has the house measured in squared feet and another variable has the house measured in squared metres.\n",
    "\n",
    "|  | Price (millions) | House Size (ft<sup>2</sup>) | House Size (m<sup>2</sup>) | ... |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| 1 | 1.5 | 2152.78 | 200 | ... |\n",
    "| 2 | 2.7 | 4305.56 | 400 | ... |\n",
    "| 3 | 3 | 5166.68 | 480 | ... |\n",
    "| 4 | 5 | 6458.35 | 600 | ... |\n",
    "| 5 | 1.75 | 3229.17 | 300 | ... |\n",
    "| 6 | 1 | 1614.59 | 150 | ... |\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Feature selection can also help reduce **overfitting** in a model. Simply put, a model is said to overfit if it we have have selected the parameters and features of the model that fits the *training set of data* (sample data used to estimate model properties) with great accuracy but this model performs poorly on a *test set of data* (a different sample of the data). By reducing overfitting, the model would be more robust to better estimate new data. We look for a parsimonious model; one that does not have much predictor or independent variables, yet can generally estimate the dependent variable.\n",
    "\n",
    "We generally try different models and see how well they perform against each other based on certain performance measures such as R<sup>2</sup>, adjusted R<sup>2</sup>, AIC, BIC or mean squared error.\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "In regression analysis, some of the basic methods we can use are an exhaustive search of all possible models and determine which one has the best performance measure score, or we could use automatic methods such as the greedy algorithms (Examples include forward selection, backward elimination or bidirectional variants of each).\n",
    "\n",
    "The exhaustive search should provide the best model based on the performance criteria, but for datasets with many independent variables, this can be computationally expensive to run. The greedy algorithms run faster but may not necessarily find the best overall model.\n",
    "\n",
    "The forward selection starts from the simplest model, either 0 or 1 independent variable(s) (or what the user predefines). It then works its way to the full model with all the variables (or what the user predefines) by adding one variable at a time. In each step of adding variables, the model with the best performance measure is kept and this value is stored. Even if adding a variable may reduce the score, we continue to add and kept track of the best model found in each step. In the end, the model with the best score out of all steps of the scores is the one selected. Backwards elimination is just the reverse, as you start off with the full model and eliminate variables to get to the simplest model; the best score and features in each step are also kept and evaluated at the end. Bidirectional variants of these involve the option of adding/eliminating variables (even if they were previously eliminated) in each step.\n",
    "\n",
    "\n",
    "We can conduct these methods through the statistical software. In R, the \"leaps\" library can be used. In Python, \"mlxtend.feature_selection\" can be used.\n",
    "\n",
    "We will run through an example for Regression Analysis using punt data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "We use the mlxtend package to run feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# install the mlxtend if you haven't already.\n",
    "# pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os # OS routines for NT or Posix depending on what system we're on.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS   # What will conduct the best fit selection\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS   # What will conduct the forward selection, backward elimination and bidirectional elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selction Example:\n",
    "punt = pd.read_csv('Datasets for Regression & Classification/punting.txt', sep=\"\\t\", header='infer')\n",
    "punt\n",
    "\n",
    "# ?pd.read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set X and y\n",
    "X = punt[[\"Hang\", \"R_Strength\", \"L_Strength\", \"R_Flexibility\", \"L_Flexibility\", \"O_Strength\"]]\n",
    "y = punt[[\"Distance\"]]\n",
    "\n",
    "# Perform the Best Fit via the Exhaustive Search:\n",
    "lr = LinearRegression()\n",
    "efspunt = EFS(lr, min_features=1, max_features=6, scoring='neg_mean_squared_error', print_progress=True, cv=2)\n",
    "\n",
    "# Create an efs fit\n",
    "efspunt.fit(X, y)\n",
    "\n",
    "print('Best negative mean squared error: %.2f' % efspunt.best_score_)\n",
    "\n",
    "## Print the variable names of the best features \n",
    "print('Variables to select:', efspunt.best_feature_names_)\n",
    "\n",
    "## Print the index in X of the best features \n",
    "print('Index variables to select in X:', efspunt.best_idx_) # Note that the columns go from 0 to 5.\n",
    "\n",
    "# To see the exhaustive search\n",
    "#print(efspunt.subsets_)\n",
    "\n",
    "# ?EFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the Sequential Forward Selection Search:\n",
    "lr = LinearRegression()\n",
    "sfspunt = SFS(lr, k_features=\"best\", forward=True, floating = False, scoring='neg_mean_squared_error', cv=2)\n",
    "# notice forward=True and floating=False\n",
    "# To get the floating variant of this, put floating = True.\n",
    "\n",
    "# Create a sfs fit\n",
    "sfspunt.fit(X, y)\n",
    "\n",
    "#print('Best negative mean squared error: %.2f' % sfspunt.best_score_)\n",
    "\n",
    "## Print the variable names of the best features \n",
    "#print('Variables to select:', sfspunt.best_feature_names_)\n",
    "\n",
    "## Print the index in X of the best features \n",
    "#print('Index variables to select in X:', sfspunt.best_idx_) # Note that the columns go from 0 to 5.\n",
    "\n",
    "# To see the forward search\n",
    "#print(sfspunt.subsets_)\n",
    "a = sfspunt.subsets_\n",
    "n = []\n",
    "o = []\n",
    "\n",
    "# Compute the mean cross validation scores\n",
    "for i in np.arange(1,7): # going from 1 variable to 6 variables since 'a' is indexed from 1 to 6\n",
    "    n.append(-(a[i]['avg_score']))  # n is just the mean of the cv mean_squared_error scores across each forward pass.\n",
    "m=np.arange(1,7) # m is the x axis for the plot, i.e. the number of variables in the model.\n",
    "\n",
    "\n",
    "# Plot the CV scores vs the number of features\n",
    "fig1=plt.plot(m,n)\n",
    "fig1=plt.title('Mean CV Scores vs No. of Features')\n",
    "fig1.figure.savefig('Images for Regression & Classification Session/sfsfig.png', bbox_inches='tight')\n",
    "\n",
    "#\n",
    "print(pd.DataFrame.from_dict(sfspunt.get_metric_dict(confidence_interval=0.90)).T)\n",
    "\n",
    "# because the index in 'a' corresponds to the number of variables,\n",
    "# we just have to get the number of variables in the index of the best feature combination\n",
    "print(\"No of features=\", len(sfspunt.k_feature_idx_)) \n",
    "\n",
    "# Get the features indices for the best forward fit\n",
    "b=sfspunt.k_feature_idx_ # Variable index in X\n",
    "print(b)\n",
    "\n",
    "# Index the column names. \n",
    "# Features from forward fit\n",
    "print(\"Features selected in forward fit\")\n",
    "print(sfspunt.k_feature_names_) # Variable selected\n",
    "\n",
    "# ?SFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the Sequential Backward Selection Search:\n",
    "lr = LinearRegression()\n",
    "sbspunt = SFS(lr, k_features=\"best\", forward=False, floating = False, scoring='neg_mean_squared_error', cv=2)\n",
    "# notice forward=False and floating=False\n",
    "# To get the floating variant of this, put floating = True.\n",
    "\n",
    "\n",
    "# Create a sfs fit\n",
    "sbspunt.fit(X, y)\n",
    "\n",
    "#print('Best negative mean squared error: %.2f' % sbspunt.best_score_)\n",
    "\n",
    "## Print the variable names of the best features \n",
    "#print('Variables to select:', sbspunt.best_feature_names_)\n",
    "\n",
    "## Print the index in X of the best features \n",
    "#print('Index variables to select in X:', sbspunt.best_idx_) # Note that the columns go from 0 to 5.\n",
    "\n",
    "# To see the forward search\n",
    "#print(sbspunt.subsets_)\n",
    "a = sbspunt.subsets_\n",
    "n = []\n",
    "o = []\n",
    "\n",
    "# Compute the mean cross validation scores\n",
    "for i in np.arange(1,7): # going from 1 variable to 6 variables since 'a' is indexed from 1 to 6\n",
    "    n.append(-(a[i]['avg_score']))  # n is just the mean of the cv mean_squared_error scores across each backward pass.\n",
    "m=np.arange(1,7) # m is the x axis for the plot, i.e. the number of variables in the model.\n",
    "\n",
    "\n",
    "# Plot the CV scores vs the number of features\n",
    "fig1=plt.plot(m,n)\n",
    "fig1=plt.title('Mean CV Scores vs No. of Features')\n",
    "fig1.figure.savefig('Images for Regression & Classification Session/sbsfig.png', bbox_inches='tight')\n",
    "\n",
    "#\n",
    "print(pd.DataFrame.from_dict(sbspunt.get_metric_dict(confidence_interval=0.90)).T)\n",
    "\n",
    "# because the index in 'a' corresponds to the number of variables,\n",
    "# we just have to get the number of variables in the index of the best feature combination\n",
    "print(\"No of features=\", len(sbspunt.k_feature_idx_)) \n",
    "\n",
    "# Get the features indices for the best backward fit\n",
    "b=sbspunt.k_feature_idx_ # Variable index in X\n",
    "print(b)\n",
    "\n",
    "# Index the column names. \n",
    "# Features from backward fit\n",
    "print(\"Features selected in backward fit\")\n",
    "print(sbspunt.k_feature_names_) # Variable selected\n",
    "\n",
    "# ?SFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Sources:**\n",
    "- For Python Code (https://www.r-bloggers.com/practical-machine-learning-with-r-and-python-part-3/)\n",
    "- Additional information on feature selection were obtained through UWI St. Augustine (STAT 6120 Course Notes and R scripts) and Wikipedia (https://en.wikipedia.org/wiki/Feature_selection#Correlation_feature_selection)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
